# SLM with ultravox & llama3.1, trained wtih knowledge distillation.
exp_name: "patrick.2024-09-10-ultravox.weights-test.1"

# Make sure to accept the license agreement on huggingface hub
text_model: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
audio_model: "openai/whisper-medium"


loss_config:
  # Choose from ["KL_Divergence", "CrossEntropy"], default is "KL_Divergence"
  loss_function: "KL_Divergence"

# Temporarily remove heysquad_human from val_sets as it causes the training to fail.
val_sets: ["anyinstruct", "soda", "peoplespeech"]

batch_size: 24
max_steps: 3000 # x8x24 = 192,000

data_sets: ["anyinstruct"]
data_dicts:
# continuation
  - path: "fixie-ai/gigaspeech"
    name: "xl-empty-audio-removed"
    splits:
      - "train"
    user_template: "Continue the following text using less than 50 words:\n\n<|audio|>"
    assistant_template: "{{ continuation }}"
    transcript_template: "{{ text_proc.format_asr_text(text) }}"
    weight: 2
    num_samples: 2000 
    total_samples: 8266422
  
  - path: "fixie-ai/wenetspeech"
    name: "L_fixed"
    splits: 
      - "train" 
    user_template: "Continue the following text using less than 50 words:\n\n<|audio|>"
    assistant_template: "{{ continuation }}"
    transcript_template: "{{ text_proc.format_asr_text(text) }}"
    weight: 1 
    num_samples: 1000
    total_samples: 14621415
  