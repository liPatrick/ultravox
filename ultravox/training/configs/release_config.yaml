# SLM with ultravox & llama3.1, trained wtih knowledge distillation.
exp_name: "ultravox-v0_4"

# Make sure to accept the license agreement on huggingface hub
text_model: "meta-llama/Meta-Llama-3.1-8B-Instruct"
audio_model: "openai/whisper-medium"


loss_config:
  # Choose from ["KL_Divergence", "CrossEntropy"], default is "KL_Divergence"
  loss_function: "KL_Divergence"

# Temporarily remove heysquad_human from val_sets as it causes the training to fail.
val_sets: ["anyinstruct", "soda", "peoplespeech"]

batch_size: 24
max_steps: 0 # x8x24 = 2,764,800
num_epochs: 1

interleave_datasets:
  stop_strategy: "LAST_EXHAUSTED"
  datasets_with_multiplier: 
    # continuation
    - dataset: 
        path: "fixie-ai/librispeech_asr"
        name: "clean"
        splits:
          - "train.100" # 28_539 samples
          - "train.360" # 104_014 samples
        user_template: "Continue the following text using less than 50 words:\n\n<|audio|>"
        assistant_template: "{{ continuation }}"
        transcript_template: "{{ text }}"
        total_samples: 132553
        num_samples: 1000
      multiplier: 1
    - dataset:
        path: "fixie-ai/librispeech_asr"
        name: "other"
        splits:
          - "train.500" # 148_688 samples
        user_template: "Continue the following text using less than 50 words:\n\n<|audio|>"
        assistant_template: "{{ continuation }}"
        transcript_template: "{{ text }}"
        total_samples: 148688
        num_samples: 1000        
      multiplier: 1
    